{"name":"The Data Scientist's Guide to Apache Spark","tagline":"Best practices of using Spark for practicing data scientists in the context of a data scientist’s standard workflow.","body":"# The Data Scientist's Guide to Apache Spark\r\n\r\n[![Join the chat at https://gitter.im/Jay-Oh-eN/data-scientists-guide-apache-spark](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/Jay-Oh-eN/data-scientists-guide-apache-spark?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Binder](http://mybinder.org/badge.svg)](http://mybinder.org/repo/zipfian/data-scientists-guide-apache-spark)\r\n\r\nThis repo contains notebook exercises for a workshop teaching the best practices of using Spark for practicing data scientists in the context of a data scientist’s standard workflow. By leveraging Spark’s APIs for Python and R to present practical applications, the technology will be much more accessible by decreasing the barrier to entry.\r\n\r\n## Materials\r\n\r\nFor the workshop (and after) we will use a Gitter chatroom to keep the conversation going: https://gitter.im/Jay-Oh-eN/data-scientists-guide-apache-spark.\r\n\r\nAnd/or please do not hesitate to reach out to me directly via email at jonathan@galvanize.com or over twitter @clearspandex\r\n\r\nThe presentation can be found on Slideshare [here](http://www.slideshare.net/jonathandinu/the-data-scientists-guide-to-apache-spark).\r\n\r\n\r\n## Prerequisites\r\n\r\nPrior experience with Python and the scientific Python stack is beneficial.  Also knowledge of data science models and applications is preferred.  This will not be an introduction to Machine Learning or Data Science, but rather a course for people proficient in these methods on a small scale to understand how to apply that knowledge in a distributed setting with Spark.\r\n\r\n### Setup\r\n\r\n* [Local Installation](http://www.slideshare.net/jonathandinu/the-data-scientists-guide-to-apache-spark/48)\r\n* [Cluster Deployment](http://www.slideshare.net/jonathandinu/the-data-scientists-guide-to-apache-spark/69)\r\n\r\n#### SparkR with a Notebook\r\n\r\n1. Install [IRKernel](https://github.com/IRkernel/IRkernel)\r\n  \r\n```r\r\ninstall.packages(c('rzmq','repr','IRkernel','IRdisplay'), repos = c('http://irkernel.github.io/', getOption('repos')))\r\n\r\nIRkernel::installspec()\r\n```\r\n\r\n2. Set [environment variables](https://github.com/apache/spark/tree/master/R#using-sparkr-from-rstudio):\r\n\r\n```\r\n# Example: Set this to where Spark is installed\r\nSys.setenv(SPARK_HOME=\"/Users/[username]/spark\")\r\n\r\n# This line loads SparkR from the installed directory\r\n.libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"), .libPaths()))\r\n\r\n# if these two lines work, you are all set\r\nlibrary(SparkR)\r\nsc <- sparkR.init(master=\"local\")\r\n```\r\n\r\n## Data\r\n\r\n`link = 'http://hopelessoptimism.com/static/data/airline-data'`\r\n\r\nThe notebooks use a few datasets.  For the DonorsChoose data, you can read the documentation [here](http://data.donorschoose.org/) and download a zip (~0.5 gb) from: http://hopelessoptimism.com/static/data/donors_choose.zip\r\n\r\n## IPython Console Help\r\n\r\nQ: How can I find out all the methods that are available on DataFrame?\r\n\r\n- In the IPython console type `sales.[TAB]`\r\n\r\n- Autocomplete will show you all the methods that are available.\r\n\r\n- To find more information about a specific method, say `.cov` type `help(sales.cov)`\r\n\r\n- This will display the API documentation for that method.\r\n\r\n## Spark Documentation\r\n\r\nQ: How can I find out more about Spark's Python API, MLlib, GraphX,\r\nSpark Streaming, deploying Spark to EC2?\r\n\r\n- Go to <https://spark.apache.org/docs/latest>\r\n\r\n- Navigate using tabs to the following areas in particular.\r\n\r\n- Programming Guide > Quick Start, Spark Programming Guide,\r\n  Spark Streaming, DataFrames and SQL, MLlib, GraphX, SparkR.\r\n\r\n- Deploying > Overview, Submitting Applications, Spark Standalone,\r\n  YARN, Amazon EC2.\r\n\r\n- More > Configuration, Monitoring, Tuning Guide.\r\n\r\n## References\r\n\r\n### Setup\r\n\r\n* [Spark on Windows 7](http://nishutayaltech.blogspot.in/2015/04/how-to-run-apache-spark-on-windows7-in.html)\r\n\r\n### History of Computing\r\n\r\n* [Why CPUs aren't getting any faster](http://www.technologyreview.com/view/421186/why-cpus-arent-getting-any-faster/)\r\n* [Hadoop: A brief History](research.yahoo.com/files/cutting.pdf)\r\n* [The State of Spark: And where we are going next](https://spark-summit.org/2013/wp-content/uploads/2013/10/Zaharia-spark-summit-2013-matei.pdf)\r\n* https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50\r\n\r\n### Original Papers\r\n\r\n* [Matei's Thesis](http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf)\r\n* [Original (RDD) Paper](https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf)\r\n\r\n### Data Science with Spark\r\n\r\n* http://blog.cloudera.com/blog/2015/07/how-to-do-data-quality-checks-using-apache-spark-dataframes/\r\n\r\n### Distributed Computing\r\n\r\n* [Distributed Systems for Fun and Profit](http://book.mixu.net/distsys/intro.html)\r\n* [Resilience Engineering: Learning to Embrace Failure](http://queue.acm.org/detail.cfm?id=2371297)\r\n* [Chaos Monkey](https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey)\r\n\r\n### Spark Internals\r\n\r\n* [PySpark Internals](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals)\r\n* [A deeper understanding of Spark's internals](https://spark-summit.org/2014/wp-content/uploads/2014/07/A-Deeper-Understanding-of-Spark-Internals-Aaron-Davidson.pdf)\r\n\r\n### Spark Performance\r\n\r\n* [Tuning and Debugging in Apache Spark](http://www.slideshare.net/pwendell/tuning-and-debugging-in-apache-spark)\r\n* [`reduceByKey` vs `groupByKey`](https://github.com/databricks/spark-knowledgebase/blob/master/best_practices/prefer_reducebykey_over_groupbykey.md)\r\n* [Advanced Spark](https://databricks-training.s3.amazonaws.com/slides/advanced-spark-training.pdf)\r\n* [What's the difference between `cache()` and `persist()`](http://stackoverflow.com/questions/26870537/spark-what-is-the-difference-between-cache-and-persist)\r\n* [Monitoring and Instrumentation](http://spark.apache.org/docs/latest/monitoring.html)\r\n\r\n### Spark Deployment\r\n\r\n* [A Tale of two clusters: Mesos vs. YARN](http://radar.oreilly.com/2015/02/a-tale-of-two-clusters-mesos-and-yarn.html)\r\n\r\n### Plotly + Spark\r\n\r\n* https://plot.ly/ipython-notebooks/apache-spark/\r\n* https://plot.ly/python/ipython-notebooks/\r\n* https://plot.ly/python/matplotlib-to-plotly-tutorial/#6.1-Matplotlib-to-Plotly-conversion-basics\r\n\r\n### word2Vec\r\n\r\n> The word2vec tool takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words. The resulting word vector file can be used as features in many natural language processing and machine learning applications.\r\n\r\n#### Theory/Application\r\n\r\n* [Efficient Estimation of Word Representations in\r\nVector Space](http://arxiv.org/pdf/1301.3781.pdf)\r\n* [Distributed Representations of Words and Phrases\r\nand their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\r\n* [Distributed Representations of Sentences and Documents](http://arxiv.org/pdf/1405.4053v2.pdf)\r\n* [deeplearning4j tutorial (with applications)](http://deeplearning4j.org/word2vec.html)\r\n* [Modern Methods for Sentiment Analysis](https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis)\r\n* [word2vec: an introduction](http://www.folgertkarsdorp.nl/word2vec-an-introduction/)\r\n\r\n#### Tools\r\n\r\n* [Spark MLlib (Python, Scala)](http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec)\r\n* [deeplearning4j (Java)](http://deeplearning4j.org/word2vec.html)\r\n* [gensim: word2vec and doc2vec (Python)](https://radimrehurek.com/gensim/models/word2vec.html)\r\n\r\nBooks on Spark\r\n--------------\r\n\r\n- Learning Spark: Lightning-Fast Big Data Analytics    \r\n  By Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia    \r\n  Publisher: O'Reilly Media, June 2014    \r\n  <http://shop.oreilly.com/product/0636920028512.do>    \r\n  Introduction to Spark APIs and underlying concepts.\r\n\r\n- Spark Knowledge Base    \r\n  By Databricks, Vida Ha, Pat McDonough    \r\n  Publisher: Databricks    \r\n  <http://databricks.gitbooks.io/databricks-spark-knowledge-base>    \r\n  Spark tips, tricks, and recipes.    \r\n\r\n- Spark Reference Applications    \r\n  By Databricks, Vida Ha, Pat McDonough    \r\n  Publisher: Databricks    \r\n  <http://databricks.gitbooks.io/databricks-spark-reference-applications>    \r\n  Best practices for large-scale Spark application architecture.\r\n  Topics include import, export, machine learning, streaming.\r\n\r\nLearning Scala\r\n--------------\r\n\r\n- Scala for the Impatient    \r\n  by Cay S. Horstmann    \r\n  Publisher: Addison-Wesley Professional, March 2012    \r\n  <http://www.amazon.com/Scala-Impatient-Cay-S-Horstmann/dp/0321774094>    \r\n  Concise, to the point, and contains good practical tips on using Scala. \r\n\r\nVideo Tutorials\r\n---------------\r\n\r\n- Spark Internals    \r\n  By Matei Zaharia (Databricks)    \r\n  <https://www.youtube.com/watch?v=49Hr5xZyTEA>    \r\n\r\n- Spark on YARN    \r\n  By Sandy Ryza (Cloudera)    \r\n  <https://www.youtube.com/watch?v=N6pJhxCPe-Y>    \r\n\r\n- Spark Programming    \r\n  By Pat McDonough (Databricks)    \r\n  <https://www.youtube.com/watch?v=mHF3UPqLOL8>    \r\n\r\nCommunity\r\n---------\r\n\r\n- Community    \r\n  <https://spark.apache.org/community.html>    \r\n  Spark's community page lists meetups, mailing-lists, and upcoming\r\n  Spark conferences.\r\n\r\n- Meetups    \r\n  <http://spark.meetup.com/>    \r\n  Spark has meetups in the Bay Area, NYC, Seattle, and most major\r\n  cities around the world.\r\n\r\n- Mailing Lists    \r\n  <https://spark.apache.org/community.html>    \r\n  The user mailing list covers issues and best practices around using\r\n  Spark. The dev mailing list is for people who want to contribute to\r\n  Spark.\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}